# 项目介绍

## 基于强化学习的足式机器人运动控制

### 背景

毕业设计，从18年5月到毕业，也就是2020年3月份。主要做仿真，人形或四足，通过与环境的交互，学习如何站立或行走，实物可以尝试实现。

### 角色

单人

### 技术细节

#### 环境

Ubuntu，python3，tensorflow，OpenAI Gym，mujoco

#### 算法

先用 DQN 做了离散动作的输出，一阶倒立摆，控制左右移动。然后跟着 OpenAI 的 baseline 学习了一下，实现了一下 PPO，

### 效果

做 20 个自由度的人形机器人控制，但是学了两天发现不收敛，应该是里面有些 trick 还没掌握，由于同时还在参加比赛，所以暂时搁置了。

### 说明

还没有做的很完善，但是放在简历的项目中的第一个位置也是有原因的，主要是由于我对强化学习特别感兴趣，也认为强化学习正在飞速发展，且我自己愿意投身强化学习。但是实在是怕找不到工作，所以期间刷了两个比赛，填补一下简历，同时也证明一下自己。



## 数字制造

### 背景

阿里云天池的比赛，从1月到2月，共一个多月。做了两个部分，一个是根据生产过程中的加工参数，预测收率（反应异烟酸质量好坏），另一个是根据自己的模型，寻找最优的加工参数使得收率最大。

### 角色

2人队，队友是我高中同学，但是都是抱着学习的心态参加的，而且都是新手，所以基本上两人独立做的，最后我的效果较好便采用了我的方案。

### 技术细节

（1）收率预测

+ 数据预处理：删除大量缺失值、类别单一值、缺失值填充、时间处理（字符串变连续的值）

+ 特征构建：根据参数物理意义，构建新特征，比如根据两个时间，计算时间差，根据两种不同的填充物的量，计算比例等。

+ 特征选择：尝试了多种方法，如计算皮尔逊相关系数，发现有些有用的特征对结果的影响是非线性的，检测不出来，还使用了 lasso 回归检测权值不为 0 的值，效果同上。使用预测模型进行前向特征选择，有效果，但是阈值不太好选取，还尝试了 sklearn 内部的特征选择，也是需要一个阈值，最后使用的后向特征选择，效果比较理想。
+ 模型：使用了多种模型、 lasso、岭回归、RF、SVM、GBDT、xgb、lgb、nn等，最后还是 xgb 和 lgb 效果最好，所以就用了这两个。
+ 融合，使用岭回归，对 lgb 和 xgb 的结果进行了融合。

（2）最优参数生成：

+ 梯度下降
+ 粒子群算法

### 遇到的难点，怎么解决

由于评测方案采用的 MSE，异常点带来的影响太大了，导致排名几乎是由异常点的处理来决定的，刚开始通过图像看比较重要的特征和收率的关系，手动寻找异常点，后来就根据数据规律，因为参数大部分为分为离散的几个类别，有个别数值只出现过1次或两次，有可能是数据抄错了，为了模型更稳定，就把某个数值出现次数过少的判断为异常值，标记为缺失值。

还要缺失值的处理，我把数据当做连续特征用了，所以填充 -1 不合适，想了一个办法，就是用其他特征的值来预测这个特征。但是在复赛中我甚至发现不填充效果更好，所以最后缺失值就没有填充。

### 效果

初赛中存在缺失值，而且我们不能看到数据，所以缺失值的处理必须是程序自动处理，不能手动更改数据，所以我可能比较占优势，排名也比较靠前，2000多个队伍排名第17。复赛加了一个最优收率预测，我的效果还比较好，但是最终排名还为止。



## 文本摘要生成

### 背景

字节跳动的比赛，机器阅读文章，自动生成标题。从18年8月到18年12月，共4个月，

### 角色

3人，实际上是两人，分别独立完成

### 技术细节

三种方案：

（1）单向 LSTM，加一种特殊的注意力机制，也是 seq2seq 架构，即编码器加解码器，分词使用 nltk，使用 glove 做词向量初始化。

（2）双向 LSTM，seq2seq 加 point network，词向量无预处理。

（3）加 policy gradient

### 难点

（1）用第一种方案时，原文中有些词在生成的词典之外，只能当做未知处理，后来生成的文本中无法出现词典之外的词。所以后来用point network了。

（2）本地显存不够，用谷歌的 colab



## 表情识别

