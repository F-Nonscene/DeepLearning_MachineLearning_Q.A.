文本摘要生成比赛总结：整个比赛为期4个月，期间我们对模型有三次较大的更新。

# 1 比赛要求

输入是英文的文章自动生成文章的标题，文章长度大部分集中在400以内最长有到1000左右，一共126万篇，官方评估使用1000个文本，评价标准使用 ROUGE（跟最长公共子序列有关），计算生成的标题和人工标题的得分作为最终得分。

# 2 第一个版本

## 数据预处理

使用 nltk 对文章进行分词，根据单词频率选择最常出现的4万的单词作为词典。使用 GloVe.6B 做为每个单词向量的初始化（也就是 Embedding 层的初始化）

## 模型

最初，使用是一种比较简单的模型，典型的 Seq2Seq 架构，也是一种 encoder decoder 的结构，使用单向       LSTM，encoder 有三层，decoder有1层。截取文章的前400个单词若不足补充特殊字符，经过 Embedding 获取词向量，然后一个一个通过 encoder 的LSTM输入，所有单词输入完成后，encoder 的最后一层 LSTM 的内部的状态送给 decoder 的 LSTM。Decoder 的第一个输入为开始标记的特殊字符，然后输入正确答案的上一个单词向量，输出会经过一个 attention 操作。每个时间步的输出会保存下来，将前 40 个长度的向量作为 key，后256-40个长度的向量作为 value，然后当前 decoder 输出的 key 和之前 encoder 的所有输出的 key 做点乘，相当于衡量相似度，然后将点乘得到的结果当做权值对每个 encoder 的输出的 value 做加权求和，得到 context vector，再将 context vector、decoder 当前输出的 value 一同送入一个全连接，全连接输出经过一个 softmax 输出词典中每个单词的概率，根据概率 sample 单词完成一个单词的输出，重复此操作直到 decoder 输出结束标志特殊字符或输出长度超过最大限度。

# 第二个版本

## 之前存在的问题

最主要的一个问题就是新闻中出现很多单词不在词典中，而且很多专有名词，如地名、人名等经常会出现在文章和标题中，但是第一个版本对这些单词在输入的时候统统当做 unknown 处理，而且输出也不会输出这些单词，所以想办法在这个问题上进行改进。

## Pointer network

Pointer network 也是参考一篇论文，思想是利用注意力来判断应该输出那个单词，比如某个人名不在字典里，那么输入的时候增加一个特殊标记表示这个单词，然后该输出这个人名的时候根据他在文中的位置和前后关系，可以将这个单词的注意力调的特别大，然后直接输出该单词。

##  模型

首先是基本结构进行了更改，encoder 和 decoder 均换成了单层的双向 LSTM，embeding 层也没有用预训练的权重。attention 也进行了一些改变，每个 LSTM 的输出同时为 key 和 value，根据 key 获得权值时是将 decoder 当前输出和所有的 key 送入一个全连接层得到的，后面操作同第一个版本，然后生成每个单词的概率。

然后加入生成单词的概率 p_gen，p_gen 概率大说明用之前的网咯得到的单词作为最终的选择，p_gen 概率小说明我们根据对 encoder 的单词的注意力来选择单词。最终的概率为使用 p_gen 对两种方法得到的生成单词的概率进行加权求和。使用加权求和后的概率进行抽样得到最终输出单词。

# 第三版本

## 之前版本存在问题

训练时，decoder 的输入，为正确答案的上一个单词，而在测试或解码时输入为自己的上一个输出，这会导致一个偏差，一般称为 exposure bias，为了解决这个问题，我们引入了强化学习。

## motivation

就是一个有 baseline 的 PG，把之前得到的句子当做 baseline，然后再用和测试时一样的方法得到一个句子，若该句子的 ROUGE 评分比 baseline 高，则增加输出该句子的概率，反之减少。