# 机器学习、深度学习、自然语言处理复习总结

##　机器学习

### 特征工程做的有哪些？非线性可分的情况怎么处理。

（1）特征工程指的是把原始数据转变为模型训练数据的过程，目的是过得更好的训练数据特征，使得机器学习模型逼近这个上限。包括：数据采集/清洗/采样，特征处理，特征构建，特征提取，特征选择。参考：[博客1](https://www.cnblogs.com/infaraway/p/8645133.html)，[博客2](https://www.cnblogs.com/wxquare/p/5484636.html)

（2）想到的两种方法：

+ 使用非线性函数区分。
+ 对数据进行非线性变换，将非线性问题变换为线性问题。如 SVM 中的 kernel method，将数据从现行不可分的输入空间映射到线性可分的特征空间。参考：统计学习方法 P116

### SVM 的核函数了解多少

映射函数为 phi(x): X -> H，则核函数为 K(x, z) = phi(x) phi(z)。

常用的核函数：多项式核函数、高斯核函数。详见统计学习方法 P122

### L1与L2的区别？为什么 L1 具有稀疏性？

（1）根据 Loss 函数后加的正则项不同，分为 L1 正则和 L2 正则，L1 正则为权值的一范数（权值绝对值之和），L2 正则为权值的二范数平方（权值平方和）。参考：西瓜书P253

（2）两种解释：

+ 数据计算：分别对加了正则项的 Loss 函数求导，则 L2 正则项在权值为 0 的时候导数为 0，而 L1 正则项在权值为 0 时的导数为正负 lambda，则引入 L2 正则对 Loss 函数在权值为 0 时的导数不产生影响，但是引入 L1 正则会使得 Loss 函数在权值为 0 时突变，若左右导数异号则说明此值为极小值，所以容易得到权值为 0。参考：[博客](https://blog.csdn.net/f156207495/article/details/82794151?utm_source=copy)
+ 直观理解（几何空间）：可观察二维平面内 Loss 函数与正则项的等值线，最优解为 Loss 函数与正则项之和最小的点，画图可知引入 L1 正则后，最优解在坐标轴上。参考：西瓜书 P253，[博客](https://blog.csdn.net/f156207495/article/details/82794151?utm_source=copy)

### 集成方法有哪些，区别是什么？

（1）大致分为两类：

+ 个体学习器间存在强依赖关系，必须串行生成的序列化方法。Boosting 为一个典型的代表：先使用初始训练集训练一个基学习器，然后根据其表现调整数据分布，使得先前基学习器做错的训练样本在后续受到更多的关注。其典型代表为 AdaBoost 以 Boosting 理解这个类别：后面的学习期依赖前面学习器的结果，所以不能并行训练，即必须串行训练。
+ 个体学习器之间不存在强依赖关系，可同时生成的并行化方法。代表为 Bagging 和随机森林，Bagging：每次使用自助采样法采集样本，在不同的样本训练基学习器；随机森林：除了样本随机，又引入属性（特征）随机，即每个基学习器使用的样本和特征均不同。

（2）同第一问：是否存在强依赖？是否必须串行生成。

参考：西瓜书 P173

### 介绍GB、GBDT、XGB、LGB、RF、LR原理

（1）GB（Gradient Boosting）：在迭代的时候选择梯度下降的方向来保证最后的结果最好，模型的预测函数为所有基学习器的和，每次迭代中，基学习器的目标为拟合之前模型的 Loss 函数的负梯度（在使用平方误差损失函数的回归问题和使用指数损失函数的分类问题时，负梯度等价于预测值与真实值的差）。

（2）GBDT（Gradient Boosting Decision Tree）：是 GB 中的一个特例，使用决策树作为基学习器，这里使用 CART 回归树，衡量最佳分割点时使用最小化均方差（在分类问题中使用的是基尼指数）。

（3）XGBoost：

+ 在目标函数上加了正则项，与叶节点数量和值有关。
+ Loss 函数在梯度（一阶导）的基础上增加了二阶导，用二阶泰勒公式逼近。
+ 对最佳分割点的衡量标准做了改变。
+ 在很多地方做了加速，包括寻找最佳分割点的方法、数据缺失值的处理、特征处理的并行等。

（4）LGB（LightGBM）：

+ 树木生长算法，XGB 采用按层增长的方式，LGB 直接选择最大收益的节点（但是深度需要加以限制，否则容易过拟合）。

+ 划分点搜索算法，XGB 采用对特征预排序的方法，LGB 采用直方图算法（将特征值分成许多小筒，在筒上搜索分裂点，减少计算和存储代价，获得更好的性能）。

+ 其他：直方图做差优化，支持类别特征，支持并行运算

  参考：[博客1](https://blog.csdn.net/luanpeng825485697/article/details/80236759)，[博客2](https://blog.csdn.net/huacha__/article/details/81057150)

（5）RF（Random Forest）：随机选取不同的特征和不同分布的数据训练基学习器，然后结合。

参考： [博客1](http://www.cnblogs.com/wxquare/p/5541414.html)，统计学习方法 P146 - P152，

### GBDT 与 RF 的区别：

（1）GBDT和随机森林的相同点：

+ 都是由多棵树组成
+ 最终的结果都是由多棵树一起决定

（2）GBDT和随机森林的不同点：

+ 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成
+ 组成随机森林的树可以并行生成；而GBDT只能是串行生成
+ 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来
+ 随机森林对异常值不敏感，GBDT对异常值非常敏感
+ 随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成
+ 随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能

参考：[机器学习基础](https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E7%AC%AC%E4%BA%8C%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md)

###　sigmoid 函数的导数和取值范围是多少

$$
f(x) = \frac{1}{1 + e^{-x}}
$$

导数：
$$
f(x)^{'} = f(x)(1 - f(x))
$$
取值为(0, 0.25]



### 最小二乘与极大似然函数的关系？

（1）概念：

+ 最小二乘：使用欧氏距离的平方作为度量单位，目标为预测值与真实值的距离越接近越好（预测值与真实值之差的平方和最小）。
+ 极大似然估计：从概率角度解释，目标位预测到真实值的概率越大越好，需要已知概率分布，一般假设正态分布。

（2）关系：在极大似然估计假设概率分布为正态分布时，两者是等价的，即预估计结果相同。

从概率统计的角度处理线性回归并在似然概率为高斯函数的假设下同最小二乘简历了联系
8，分类的评价标准，准确率，AUC等等
9，有的逻辑回归损失函数中为啥要加-1*m
10，欠拟合的解决方法：模型简单，加深神经网络，svm用核函数等等
11，L2 正则的本质：限制解空间范围，缩小解空间，控制模型复杂度
12，SVM 引入核函数本质？提高维度，增加模型复杂度
13，lgb，xgb，gbdt，rf，lr 等的区别
14，树模型的特征选择中除了信息增益、信息增益比、基尼指数这三个外，还有那些？
15，Sklearn中树模型输出的特征重要程度是本身的还是百分比？
16，熟悉FM算法不？
17，RF的随机性体现在哪里？它的代码中输出的特征重要程度是怎么进行计算的
18，了解哪些损失函数？区别是什么？
19，线性模型为何用的最小二乘作为损失函数而不用似然函数或者交叉熵？
20，推导 LR
21，为啥LR的输入特征一般是离散的而不是连续的？
22，了解各种优化算法不？梯度下降和随机梯度下降的区别？牛顿法和拟牛顿法的区别？为啥提出拟牛顿？因为牛顿法涉及海塞矩阵，它的逆矩阵求解很麻烦
23，KNN的使用场景
24，自我评价，自己的优缺点
25，工作中遇到的问题怎么解决的
26，家庭对你的影响
27，遇到的最遗憾的事情，最有成就的事情，最后悔的事情
28，





# 深度学习

